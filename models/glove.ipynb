{
 "cells": [
  {
   "cell_type": "code",
   "id": "2cecf9eaa8dc8c14",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T18:00:27.712460Z",
     "start_time": "2024-05-21T18:00:27.634263Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from os.path import expanduser"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:00:56.253489Z",
     "start_time": "2024-05-21T18:00:45.815489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings, we will use the 300-dimensional embeddings with 6 billion tokens\n",
    "PATH = expanduser('~/data/glove.6B.300d.txt')\n",
    "embeddings_index = load_glove_embeddings(PATH)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T18:10:03.953392Z",
     "start_time": "2024-05-21T18:10:03.949624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the embedding vector for a word\n",
    "WORD = 'brush'\n",
    "def get_embedding_vector(embeddings_index, word):\n",
    "    return embeddings_index.get(word)\n",
    "\n",
    "embedding_vector = get_embedding_vector(embeddings_index, WORD)\n",
    "print(embedding_vector.shape)"
   ],
   "id": "51db9a1a17250026",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:17:51.705838Z",
     "start_time": "2024-05-21T19:17:29.269476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_words(word, embeddings_index, top_n=5):\n",
    "    \"\"\"\n",
    "    Find the top N most similar words to a given word based on cosine similarity.\n",
    "    \n",
    "    :param word: The word for which similar words are to be found.\n",
    "    :param embeddings_index: A dictionary of word embeddings.\n",
    "    :param top_n: The number of similar words to return.\n",
    "    :return: A tuple of two lists - similar words and their cosine similarity scores.\n",
    "    \"\"\"\n",
    "    word_embedding = get_embedding_vector(embeddings_index, word)\n",
    "    if word_embedding is None:\n",
    "        return None, None\n",
    "\n",
    "    similarities = {}\n",
    "\n",
    "    for key, value in embeddings_index.items():\n",
    "        if key == word:\n",
    "            continue\n",
    "        similarities[key] = cosine_similarity([word_embedding], [value])[0, 0]\n",
    "\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    similar_words, similar_scores = zip(*similar_words) if similar_words else ([], [])\n",
    "    \n",
    "    return list(similar_words), list(similar_scores)\n",
    "\n",
    "# Example usage\n",
    "WORD = 'brush'\n",
    "similar_words, similar_scores = find_similar_words(WORD, embeddings_index)\n",
    "print(f\"Similar words to '{WORD}': {similar_words}\")\n",
    "print(f\"Similarity scores: {similar_scores}\")"
   ],
   "id": "2f858b6d68f5a1ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'brush': ['brushes', 'scrub', 'dry', 'bristle', 'spray']\n",
      "Similarity scores: [0.58875906, 0.5571601, 0.47539973, 0.45540044, 0.45286047]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning GloVe embeddings\n",
    "\n",
    "We can fine tune the GloVe embeddings by training them on a specific dataset. This can be done by using the GloVe embeddings as the initial weights of an embedding layer in a neural network and then training the network on the specific dataset. The embeddings will be updated during training, and the model will learn task-specific representations.\n"
   ],
   "id": "6bbb9570d476ac9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
